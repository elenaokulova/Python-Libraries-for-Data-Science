1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
 
В моделях с решающими деревьями переобучение возникает из-за того, что дерево запоминает большую часть данных
конкретной выборки. Поэтому главная идея - недопустить этого. В данных алгоритмах штрафуются:

- Глубина дерева. То есть ограничивается рост дерева в глубину, чтобы не дать в точности подстроиться под имеющиеся
данные. 
-  Минимальный вес листа. На основе количества информации, содержащейся в листе, принимается решение, продолжать
рост дерева или остановать. Таким образом мы указываем минимальное количество наблюдений, которое может
содержаться в листе. Если в листе будет слишком мало наблюдений, модель более подробно запомнит данные в дереве
и возникнет сильное переобучение.


2. По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?

Деревья решений "задают вопросы" для обучения на предоставленных данных. "Вопросы" задаются на основе имеющихся
признаков данных. Качество "вопроса" рассчитывается по таким критериям, как Энтропия Шенона, критерий Джинни. 
Также на основе этих критериев подсчитвается неопределенность данных (шумы). Таким образом можно рассчитать, как каждый 
признак уменьшает неопределенность данных при построении отдельного дерева. Потом суммируется 
информация по каждому дереву и берется среднее число уменьшения неопределенности по каждому признаку. 
Это число и будет важностью признака.
Один из недостатков данного метода заключается в том, что при корреляции некоторых признаков предпочтене отдается 
только одному и важностью другого принебрегает. Поэтому важно по возможности снижать высокую корреляцию
отдельных признаков.
